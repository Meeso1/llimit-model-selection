# Model Behavior Encoder

This document describes the `ModelBehaviorEncoder`, a Stage 1 model that creates meaningful vector representations (embeddings) of large language models based on their behavior, specifically their responses to given prompts.

## 1. Overview

The current `DenseNetworkModel` learns a static, unique embedding for each model ID. This approach is inflexible and cannot handle new, unseen models without a full retrain. To address this, we have implemented a two-stage training process.

-   **Stage 1 (This Model)**: The `ModelBehaviorEncoder` is a Siamese-style network trained to generate embeddings from `(prompt, response)` pairs. The goal is to create an embedding space where models with similar performance characteristics (as judged by humans) are located closer to each other.
-   **Stage 2 (Scoring Model)**: The existing `DenseNetworkModel` will be adapted to use these pre-computed, behavior-based embeddings instead of its internal `model_id` embedding layer. This will allow it to score any model, including new ones, as long as we can generate an embedding for it. *(Not yet implemented)*

The `ModelBehaviorEncoder` does **not** implement the `ModelBase` interface directly but is a self-contained component with its own preprocessing, training, and serialization logic.

## 2. Architecture

The encoder uses a two-component architecture:

-   **Frozen Text Encoder**: A sentence transformer model (from `sentence-transformers` library) that is frozen during training. It concatenates `prompt` and `response` strings with a `[SEP]` token before encoding. The outputs are cached during training for efficiency.
-   **Trainable Dense Network**: A configurable dense neural network (with ReLU activations and dropout) that transforms the frozen text encoder embeddings into a learned embedding space. The architecture is specified via `hidden_dims` parameter (e.g., `[256, 128]`).
-   **`ModelBehaviorEncoder`** (`src/models/model_behavior_encoder.py`): The main class that orchestrates the components. It handles the training loop, preprocessing, and serialization.

This design keeps the text encoder frozen (avoiding expensive fine-tuning) while still allowing the model to learn a task-specific embedding space through the trainable dense layers.

## 3. Data Flow and Models

Data structures in `src/data_models/behavior_encoder_types.py` handle the data flow for this model.

-   **Input Data**: The model trains on triplets of `(prompt, response)` pairs. A `BehaviorEncoderTrainingTriplet` dataclass represents this, containing anchor, positive, and negative prompt-response pairs.
-   **Preprocessed Data**: A `PreprocessedBehaviorEncoderData` dataclass holds the list of training triplets after filtering and preprocessing.
-   **Output Data**: The primary output is a fixed-size embedding vector (`np.ndarray`). The `encode()` method returns embeddings for multiple (prompt, response) pairs, while `compute_model_embedding()` returns a single averaged embedding for a model.

## 4. Training Process

### 4.1. Two-Stage Approach

Training must be strictly separated into two stages:

1.  **Stage 1**: Train the `ModelBehaviorEncoder` using a triplet margin loss until its validation metrics converge. The trained encoder is then saved.
2.  **Stage 2**: Freeze the Stage 1 encoder. Use it to pre-compute an average embedding for every model in the dataset. Train the Stage 2 scoring model on these static, pre-computed embeddings.

Jointly training both models is not viable as it would lead to an unstable, constantly shifting embedding space for the scoring model.

### 4.2. Triplet Selection Strategy

The training process uses `TripletMarginLoss`. The key is the strategic selection of triplets to inject human preference data into the embedding space. Given a pairwise comparison `(prompt, response_A, model_A, response_B, model_B, winner=model_A)`:

-   **Anchor (`A`)**: The winning pair: `(prompt, response_A)`.
-   **Negative (`N`)**: The losing pair from the same comparison: `(prompt, response_B)`. This provides a strong, direct preference signal. If no losing pair is available, `both_bad` pairs can serve as fallback negatives.
-   **Positive (`P`)**: A hybrid strategy is used for selecting the positive pair:
    -   **Identity Positive (80% of samples by default)**: A different `(prompt', response')` pair generated by the same model as the anchor (`model_A`). This teaches the encoder to recognize a model's unique behavioral signature.
    -   **Performance Positive (20% of samples by default)**: A winning pair `(prompt'', response_C)` from a different model (`model_C`). This encourages the model to learn a general concept of a "good response", pushing winning embeddings from different models closer together.

**For tie entries:**
- Tie pairs are used as positives for each other
- `both_bad` pairs serve as negatives for tie anchors

The ratio between identity and performance positives is configurable via the `identity_positive_ratio` parameter. All random selections use a seeded random number generator for reproducibility.

### 4.3. Validation and Goal Metrics

-   **Validation Split**: The model supports validation via the `ValidationSplit` parameter. Currently, a simple random split of the training data is performed. A more robust approach would be to split by `user_prompt` to ensure the model is evaluated on its ability to generalize to unseen prompts. *(Prompt-based splitting not yet implemented)*
-   **Goal Metrics**:
    1.  **Validation Loss**: The primary metric to monitor for convergence. This includes both triplet loss and regularization loss.
    2.  **Triplet Accuracy**: For a validation triplet `(A, P, N)`, we check if `distance(A, P) < distance(A, N)`. The percentage of triplets satisfying this condition is an intuitive measure of how well the embedding space is structured.

### 4.4. Training Length

Training is defined by a fixed number of **epochs**. We do not use a "quality threshold" for stopping, but we monitor the validation `Triplet Accuracy` and validation loss to identify the best-performing epoch and prevent overfitting.

## 5. Data Considerations

### 5.1. Preprocessing and Caching

The `BehaviorEmbeddingPreprocessor` (`src/preprocessing/behavior_embedding_preprocessor.py`) is responsible for:
-   Reading the raw `TrainingData`.
-   Applying filtering rules.
-   Constructing the training triplets based on the defined strategy.
-   Implementing a caching mechanism identical to `PromptEmbeddingPreprocessor` to save preprocessed triplets to disk and avoid re-computation.

The preprocessor uses a `Jar` to cache preprocessed data based on a hash of the dataset and preprocessing parameters.

### 5.2. Data Filtering Rules

Before triplet construction, the raw data is filtered:

-   **Filter Rare Models**: Models that appear in fewer than a threshold number of comparisons (default: 20, configurable via `min_model_comparisons`) are excluded from the training set. Their data is likely too sparse to learn a stable, representative embedding.
-   **Filter Invalid Entries**: Entries with empty prompts or responses are discarded.

## 6. API and Usage

### 6.1. Initialization

The `ModelBehaviorEncoder` can be initialized with the following parameters:

```python
from src.models.model_behavior_encoder import ModelBehaviorEncoder
from src.models.optimizers.adamw_spec import AdamWSpec

encoder = ModelBehaviorEncoder(
    encoder_model_name="all-MiniLM-L6-v2",  # Sentence transformer model (frozen)
    hidden_dims=[256, 128],  # Trainable dense layer dimensions
    optimizer_spec=AdamWSpec(learning_rate=0.001),  # Configurable optimizer
    triplet_margin=0.2,  # Margin for triplet loss
    regularization_weight=0.01,  # Weight for KL-divergence regularization
    min_model_comparisons=20,  # Minimum comparisons per model
    identity_positive_ratio=0.8,  # Ratio of identity vs performance positives
    preprocessor_seed=42,  # Random seed for reproducible preprocessing
    print_every=1,  # Print progress every N epochs (None = no printing)
)
```

### 6.2. Public Method Signatures

The `ModelBehaviorEncoder` class exposes the following public methods:

```python
from src.data_models.behavior_encoder_types import PromptResponsePair

def train(
    self,
    data: TrainingData,
    validation_split: ValidationSplit | None = None,
    epochs: int = 10,
    batch_size: int = 32
) -> None:
    """Trains the model on the given data."""
    ...

def encode(
    self,
    pairs: list[PromptResponsePair],
    batch_size: int = 32
) -> np.ndarray: # [n_samples, embedding_dim]
    """Encodes a list of (prompt, response) pairs into embeddings."""
    ...

def compute_model_embedding(
    self,
    pairs: list[PromptResponsePair],
    batch_size: int = 32
) -> np.ndarray: # [embedding_dim]
    """
    Computes a single, representative embedding for a model by
    averaging the embeddings of its (prompt, response) pairs.
    """
    ...

def get_state_dict(self) -> dict[str, Any]:
    """Returns a serializable state dictionary."""
    ...

@classmethod
def load_state_dict(cls, state_dict: dict[str, Any]) -> "ModelBehaviorEncoder":
    """Loads a model from a state dictionary."""
    ...
```

### 6.3. Usage Example

```python
from src.data_models.behavior_encoder_types import PromptResponsePair
from src.utils.data_split import ValidationSplit

# Training
encoder.train(
    data=training_data,
    validation_split=ValidationSplit(val_fraction=0.2, seed=42),
    epochs=20,
    batch_size=32,
)

# Encoding pairs
pairs = [
    PromptResponsePair(prompt="What is Python?", response="Python is..."),
    PromptResponsePair(prompt="Explain ML", response="Machine learning is..."),
]
embeddings = encoder.encode(pairs, batch_size=32)  # [2, embedding_dim]

# Computing model embedding
model_embedding = encoder.compute_model_embedding(pairs, batch_size=32)  # [embedding_dim]

# Serialization
state_dict = encoder.get_state_dict()
# Save to file...

# Loading
encoder = ModelBehaviorEncoder.load_state_dict(state_dict)
```

### 6.4. History and Serialization

-   **Training History**: The `train` method does not return a `TrainingHistory` object. Instead, it maintains an internal list of `EpochLog` dataclasses containing loss and accuracy metrics for each epoch.
-   **Serialization**: The model is serializable via `get_state_dict` and `load_state_dict`, allowing the trained encoder to be saved and loaded independently.

## 7. Advanced Features

### 7.1. Incorporating `tie` and `both_bad` data

The implementation includes support for `tie` and `both_bad` entries:

-   **Ties**: When two models tie (`winner="tie"`), their `(prompt, response)` pairs are used as positives for each other. This pulls the embeddings of similarly high-performing models closer together.
-   **`both_bad`**: When both models are bad, their pairs serve as a rich source of negatives for any anchor-positive pair, helping the model learn a "bad response" region in the embedding space.

### 7.2. Regularization Loss

The implementation includes a KL-divergence regularization loss (VAE-style) that encourages the embedding distribution to follow a standard normal distribution. This improves generalization and prevents the embedding space from collapsing. The regularization weight is configurable via the `regularization_weight` parameter.

The total loss is computed as:
```
total_loss = triplet_loss + regularization_weight * kl_divergence_loss
```

### 7.3. Future Enhancements

Potential future improvements include:

-   **Compactness Loss**: An additional loss term that encourages all embeddings from the same model to be close to their computed mean embedding, making the representation for each model more stable.
-   **Prompt-based Validation Split**: Split validation data by `user_prompt` to ensure the model is evaluated on its ability to generalize to unseen prompts.
-   **Fine-tunable Text Encoder**: Unfreeze and fine-tune the last few layers of the sentence transformer for potentially better performance.
-   **Pluggable Text Encoder**: Add an abstract interface for text encoders (similar to `OptimizerSpecification`) to allow swapping different transformer backbones.

## 8. Implementation Status

### 8.1. Completed Components

-   **`docs/models/model_behavior_encoder.md`**: This document.
-   **`src/data_models/behavior_encoder_types.py`**: Dataclasses for data modeling (`PromptResponsePair`, `BehaviorEncoderTrainingTriplet`, `PreprocessedBehaviorEncoderData`, `ModelBehaviorEncoderOutput`).
-   **`src/preprocessing/behavior_embedding_preprocessor.py`**: Preprocessor with caching, filtering, and triplet construction.
-   **`src/models/model_behavior_encoder.py`**: Main model class with:
    -   Frozen sentence transformer for text encoding
    -   Trainable dense layers for embedding transformation
    -   Training loop with triplet loss and regularization
    -   Support for tie/both_bad entries
    -   Efficient caching of frozen encoder outputs during training

### 8.2. Pending Work

-   Update `DenseNetworkModel` to accept pre-computed embeddings from the behavior encoder (Stage 2 integration).
-   Implement prompt-based validation split for better generalization evaluation.
-   Add CLI commands for training and using the behavior encoder.
