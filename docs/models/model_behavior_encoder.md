# Model Behavior Encoder

This document outlines the design for the `ModelBehaviorEncoder`, a Stage 1 model responsible for creating meaningful vector representations (embeddings) of large language models based on their behavior, specifically their responses to given prompts.

## 1. Overview

The current `DenseNetworkModel` learns a static, unique embedding for each model ID. This approach is inflexible and cannot handle new, unseen models without a full retrain. To address this, we are introducing a two-stage training process.

-   **Stage 1 (This Model)**: The `ModelBehaviorEncoder` will be a Siamese-style network trained to generate embeddings from `(prompt, response)` pairs. The goal is to create an embedding space where models with similar performance characteristics (as judged by humans) are located closer to each other.
-   **Stage 2 (Scoring Model)**: The existing `DenseNetworkModel` will be adapted to use these pre-computed, behavior-based embeddings instead of its internal `model_id` embedding layer. This will allow it to score any model, including new ones, as long as we can generate an embedding for it.

The `ModelBehaviorEncoder` will **not** implement the `ModelBase` interface directly but will be a self-contained component with its own preprocessing, training, and serialization logic.

## 2. Architecture

The encoder will have a modular architecture to facilitate experimentation with different components.

-   **`TextEncoderBase`**: An abstract base class for the core text encoding module. This allows us to swap different transformer backbones.
-   **`SentenceTransformerEncoder`**: The initial concrete implementation of `TextEncoderBase`, using the `sentence-transformers` library. It will concatenate `prompt` and `response` strings before encoding.
-   **Pooling Layer**: A module to aggregate token-level embeddings from the text encoder into a single fixed-size vector for each `(prompt, response)` pair. Mean pooling is a sensible default.
-   **`ModelBehaviorEncoder`**: The main class that orchestrates the components. It will contain the text encoder and pooling layer, and handle the training loop.

This design allows us to easily replace the `SentenceTransformerEncoder` with a fine-tunable model from the Hugging Face Hub if needed.

## 3. Data Flow and Models

New data structures will be created in `src/data_models/behavior_encoder_types.py` to handle the data flow for this model.

-   **Input Data**: The model trains on triplets of `(prompt, response)` pairs. A `BehaviorEncoderTrainingTriplet` dataclass will represent this.
-   **Preprocessed Data**: A `PreprocessedBehaviorEncoderData` dataclass will hold the list of training triplets after any necessary filtering and preprocessing.
-   **Output Data**: The primary output is a fixed-size embedding vector (`np.ndarray`). When generating embeddings for a set of models, the output will be a dictionary mapping model names to their computed embeddings (`dict[str, np.ndarray]`).

## 4. Training Process

### 4.1. Two-Stage Approach

Training must be strictly separated into two stages:

1.  **Stage 1**: Train the `ModelBehaviorEncoder` using a triplet margin loss until its validation metrics converge. The trained encoder is then saved.
2.  **Stage 2**: Freeze the Stage 1 encoder. Use it to pre-compute an average embedding for every model in the dataset. Train the Stage 2 scoring model on these static, pre-computed embeddings.

Jointly training both models is not viable as it would lead to an unstable, constantly shifting embedding space for the scoring model.

### 4.2. Triplet Selection Strategy

The training process will use `TripletMarginLoss`. The key is the strategic selection of triplets to inject human preference data into the embedding space. Given a pairwise comparison `(prompt, response_A, model_A, response_B, model_B, winner=model_A)`:

-   **Anchor (`A`)**: The winning pair: `(prompt, response_A)`.
-   **Negative (`N`)**: The losing pair from the same comparison: `(prompt, response_B)`. This provides a strong, direct preference signal.
-   **Positive (`P`)**: We will use a hybrid strategy for selecting the positive pair:
    -   **Identity Positive (80% of samples)**: A different `(prompt', response')` pair generated by the same model as the anchor (`model_A`). This teaches the encoder to recognize a model's unique behavioral signature.
    -   **Performance Positive (20% of samples)**: A winning pair `(prompt'', response_C)` from a different model (`model_C`). This encourages the model to learn a general concept of a "good response", pushing winning embeddings from different models closer together.

### 4.3. Validation and Goal Metrics

-   **Validation Split**: We need a validation set. A simple random split of the training triplets is a start, but a more robust approach is to split by `user_prompt`. This ensures the model is evaluated on its ability to generalize to unseen prompts.
-   **Goal Metrics**:
    1.  **Validation Loss**: The primary metric to monitor for convergence.
    2.  **Triplet Accuracy**: For a validation triplet `(A, P, N)`, we check if `distance(A, P) < distance(A, N)`. The percentage of triplets satisfying this condition is an intuitive measure of how well the embedding space is structured.

### 4.4. Training Length

Training will be defined by a fixed number of **epochs**. We will not use a "quality threshold" for stopping, but we will monitor the validation `Triplet Accuracy` and validation loss to identify the best-performing epoch and prevent overfitting.

## 5. Data Considerations

### 5.1. Preprocessing and Caching

A new preprocessor, `BehaviorEmbeddingPreprocessor`, will be created in `src/preprocessing/`. It will be responsible for:
-   Reading the raw `TrainingData`.
-   Applying filtering rules.
-   Constructing the training triplets based on the defined strategy.
-   Implementing a caching mechanism identical to `PromptEmbeddingPreprocessor` to save preprocessed triplets to disk and avoid re-computation.

### 5.2. Data Filtering Rules

Before triplet construction, the raw data will be filtered:

-   **Filter Rare Models**: Models that appear in fewer than a threshold number of comparisons (e.g., < 20) will be excluded from the training set for the encoder. Their data is likely too sparse to learn a stable, representative embedding.
-   **Filter Invalid Entries**: Entries with empty prompts or responses will be discarded.

## 6. API and Usage

### 6.1. Public Method Signatures

The `ModelBehaviorEncoder` class will expose the following public methods:

```python
def train(
    self,
    data: TrainingData,
    validation_split: ValidationSplit,
    epochs: int,
    batch_size: int
) -> None:
    """Trains the model on the given data."""
    ...

def encode(
    self,
    prompts: list[str],
    responses: list[str],
    batch_size: int
) -> np.ndarray: # [n_samples, embedding_dim]
    """Encodes a list of (prompt, response) pairs into embeddings."""
    ...

def compute_model_embedding(
    self,
    prompts: list[str],
    responses: list[str],
    batch_size: int
) -> np.ndarray: # [embedding_dim]
    """
    Computes a single, representative embedding for a model by
    averaging the embeddings of its (prompt, response) pairs.
    """
    ...

def get_state_dict(self) -> dict[str, Any]:
    """Returns a serializable state dictionary."""
    ...

@classmethod
def load_state_dict(cls, state_dict: dict[str, Any]) -> "ModelBehaviorEncoder":
    """Loads a model from a state dictionary."""
    ...
```

### 6.2. History and Serialization

-   **Training History**: The `train` method will not return a `TrainingHistory` object. Instead, it will maintain an internal list of per-epoch logs (e.g., a dataclass containing loss and accuracy metrics).
-   **Serialization**: The model will be serializable via `get_state_dict` and `load_state_dict`, allowing the trained encoder to be saved and loaded independently.

## 7. Future Work & Enhancements

### 7.1. Incorporating `tie` and `both_bad` data

-   **Ties**: When two models tie (`winner="tie"`), their `(prompt, response)` pairs can be used as positives for each other. For a triplet `(Anchor, Positive, Negative)`, we can form `(response_A, response_B, some_negative_response)` and `(response_B, response_A, some_negative_response)`. This will pull the embeddings of similarly high-performing models closer together.
-   **`both_bad`**: When both models are bad, their pairs can serve as a rich source of negatives for any anchor-positive pair, helping the model learn a "bad response" region in the embedding space.

### 7.2. Advanced Loss Components

To further improve the structure of the embedding space, we could explore additional loss components:
-   **Regularization Loss (VAE-style)**: Add a KL-divergence loss term to encourage the embedding distribution to follow a prior (e.g., a standard normal distribution). This can improve generalization.
-   **Compactness Loss**: An additional loss term that encourages all embeddings from the same model to be close to their computed mean embedding, making the representation for each model more stable.

## 8. Implementation Plan

-   **`docs/models/model_behavior_encoder.md`**: This document.
-   **`src/data_models/behavior_encoder_types.py`**: Create dataclasses for data modeling.
-   **`src/models/behavior/encoder_base.py`**: Create the abstract `TextEncoderBase`.
-   **`src/preprocessing/behavior_embedding_preprocessor.py`**: Implement the new preprocessor.
-   **`src/models/behavior/model_behavior_encoder.py`**: Implement the main model class and its components.
-   Update `DenseNetworkModel` to accept pre-computed embeddings (in a later step).
