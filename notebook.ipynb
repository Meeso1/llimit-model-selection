{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "from src import data_loading\n",
    "from src.data_models.data_models import TrainingData\n",
    "\n",
    "dataset = datasets.load_dataset(\"lmarena-ai/arena-human-preference-140k\")\n",
    "data = data_loading.load_training_data_lmarena(dataset[\"train\"].to_pandas())\n",
    "#data = TrainingData(random.sample(data.entries, 10000))\n",
    "print(f\"Successfully loaded {len(data.entries)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce526da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.dn_embedding_model import DnEmbeddingModel\n",
    "from src.models.embedding_specs.finetunable_embedding_spec import FinetunableEmbeddingSpec\n",
    "from src.models.embedding_specs.frozen_embedding_spec import FrozenEmbeddingSpec\n",
    "from src.models.embedding_specs.attention_embedding_spec import AttentionEmbeddingSpec\n",
    "from src.models.optimizers.adamw_spec import AdamWSpec\n",
    "from src.utils.data_split import ValidationSplit\n",
    "\n",
    "from src.utils.timer import Timer\n",
    "#Timer.default_verbosity = None\n",
    "\n",
    "\n",
    "model = DnEmbeddingModel(\n",
    "    hidden_dims=[512, 256, 128, 64],\n",
    "    optimizer_spec=AdamWSpec(\n",
    "        learning_rate=1e-4,\n",
    "        lr_decay_gamma=0.99,\n",
    "    ),\n",
    "    balance_model_samples=True,\n",
    "    embedding_model_name=\"all-MiniLM-L6-v2\", # \"all-mpnet-base-v2\",\n",
    "    min_model_comparisons=1000,\n",
    "    seed=42,\n",
    "    print_every=1,\n",
    "    \n",
    "    # embedding_spec=FrozenEmbeddingSpec(\n",
    "    #     encoder_model_name=\"all-MiniLM-L6-v2\",\n",
    "    #     hidden_dims=[512, 256, 128, 64],\n",
    "    #     optimizer=AdamWSpec(\n",
    "    #         learning_rate=1e-4,\n",
    "    #         lr_decay_gamma=0.995,\n",
    "    #     ),\n",
    "    #     triplet_margin=0.2,\n",
    "    #     regularization_weight=0,\n",
    "    #     identity_positive_ratio=1,\n",
    "    # )\n",
    "    \n",
    "    embedding_model_epochs=100, \n",
    "    embedding_spec=AttentionEmbeddingSpec(\n",
    "        encoder_model_name=\"all-MiniLM-L6-v2\",\n",
    "        optimizer=AdamWSpec(\n",
    "            learning_rate=5e-5,\n",
    "            lr_decay_gamma=0.995,\n",
    "        ),\n",
    "        h_emb=256,\n",
    "        h_scalar=64,\n",
    "        h_pair=256,\n",
    "        d_out=128,\n",
    "        pair_mlp_layers=8,\n",
    "        num_attention_heads=8,\n",
    "        dropout=0.1,\n",
    "        temperature=0.07,\n",
    "        pairs_per_model=128,\n",
    "        models_per_batch=8,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.train(data, validation_split=ValidationSplit(val_fraction=0.2, seed=42), epochs=50, batch_size=64)\n",
    "model.save(\"new-prompt-features-short-pc-run\")\n",
    "\n",
    "print()\n",
    "model.last_timer.inspect(1)\n",
    "print()\n",
    "model.embedding_model.last_timer.inspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff823cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.plotting_utils import plot_loss, plot_accuracy\n",
    "\n",
    "history = model.get_history()\n",
    "_, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "plot_loss(axes[0, 0], history.total_loss, \"Training loss\")\n",
    "plot_loss(axes[0, 1], history.val_loss, \"Validation loss\")\n",
    "plot_accuracy(axes[1, 0], history.train_accuracy, \"Training accuracy\")\n",
    "plot_accuracy(axes[1, 1], history.val_accuracy, \"Validation accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.plotting_utils import plot_loss, plot_accuracy\n",
    "\n",
    "embedding_history = model.embedding_model._epoch_logs\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "plot_loss(axes[0, 0], [e.train_loss for e in embedding_history], \"Training loss\")\n",
    "plot_loss(axes[0, 1], [e.val_loss for e in embedding_history], \"Validation loss\")\n",
    "plot_accuracy(axes[1, 0], [e.triplet_accuracy for e in embedding_history], \"Training triplet accuracy\")\n",
    "plot_accuracy(axes[1, 1], [e.val_triplet_accuracy for e in embedding_history], \"Validation triplet accuracy\")\n",
    "# plot_accuracy(axes[2, 0], [e.nearest_neighbor_accuracy for e in embedding_history], \"Training nearest neighbor accuracy\")\n",
    "# plot_accuracy(axes[2, 1], [e.val_nearest_neighbor_accuracy for e in embedding_history], \"Validation nearest neighbor accuracy\")\n",
    "plot_accuracy(axes[2, 0], [e.train_universal_accuracy for e in embedding_history], \"Training universal accuracy\")\n",
    "plot_accuracy(axes[2, 1], [e.val_universal_accuracy for e in embedding_history], \"Validation universal accuracy\")\n",
    "\n",
    "# axes[2, 0].plot([e.train_loss for e in embedding_history], label=\"Training loss\")\n",
    "# axes[2, 0].plot([e.train_triplet_loss for e in embedding_history], label=\"Training triplet loss\")\n",
    "# axes[2, 0].plot([e.train_reg_loss for e in embedding_history], label=\"Training regularization loss\")\n",
    "# axes[2, 0].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
