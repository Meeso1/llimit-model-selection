{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c436636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import datasets\n",
    "from src import data_loading\n",
    "from src.data_models.data_models import TrainingData\n",
    "\n",
    "dataset = datasets.load_dataset(\"lmarena-ai/arena-human-preference-140k\")\n",
    "data = data_loading.load_training_data_lmarena(dataset[\"train\"].to_pandas())\n",
    "#dataset = datasets.load_dataset(\"lmsys/chatbot_arena_conversations\")\n",
    "#data = data_loading.load_training_data_chatbot_arena(dataset[\"train\"].to_pandas())\n",
    "#data = TrainingData(random.sample(data.entries, 10000))\n",
    "print(f\"Successfully loaded {len(data.entries)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.embedding_specs.attention_embedding_spec import AttentionEmbeddingSpec\n",
    "from src.models.gradient_boosting_model import GradientBoostingModel\n",
    "from src.models.optimizers.adamw_spec import AdamWSpec\n",
    "from src.utils.data_split import ValidationSplit\n",
    "\n",
    "\n",
    "prompt_invariant_model = GradientBoostingModel(\n",
    "    max_depth=4,\n",
    "    learning_rate=1e-5,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    input_features=[\"model_embedding\"],\n",
    "    balance_model_samples=True,\n",
    "    min_model_comparisons=1000,\n",
    "    seed=42,\n",
    "    print_every=5,\n",
    "    \n",
    "    # embedding_model_epochs=250, \n",
    "    # embedding_spec=AttentionEmbeddingSpec(\n",
    "    #     encoder_model_name=\"all-MiniLM-L6-v2\",\n",
    "    #     optimizer=AdamWSpec(\n",
    "    #         learning_rate=2e-5,\n",
    "    #         lr_decay_gamma=0.995,\n",
    "    #     ),\n",
    "    #     h_emb=256,\n",
    "    #     h_scalar=64,\n",
    "    #     h_pair=256,\n",
    "    #     d_out=128,\n",
    "    #     pair_mlp_layers=8,\n",
    "    #     num_attention_heads=8,\n",
    "    #     dropout=0.1,\n",
    "    #     temperature=0.07,\n",
    "    #     pairs_per_model=128,\n",
    "    #     models_per_batch=8,\n",
    "    # )\n",
    "    load_embedding_model_from=\"gradient-boosting-prompt-categories-no-embeddings\"\n",
    ")\n",
    "\n",
    "prompt_invariant_model.train(data, validation_split=ValidationSplit(val_fraction=0.2, seed=42), epochs=180, batch_size=128)\n",
    "prompt_invariant_model.save(\"gradient-boosting-model-embeddings-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51a8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.embedding_specs.attention_embedding_spec import AttentionEmbeddingSpec\n",
    "from src.models.gradient_boosting_model import GradientBoostingModel\n",
    "from src.models.optimizers.adamw_spec import AdamWSpec\n",
    "from src.utils.data_split import ValidationSplit\n",
    "\n",
    "\n",
    "gradient_boosting_model = GradientBoostingModel(\n",
    "    max_depth=4,\n",
    "    learning_rate=1e-5,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=1.0,\n",
    "    input_features=[\"model_embedding\", \"prompt_embedding\", \"prompt_categories\", \"prompt_features\"],\n",
    "    balance_model_samples=True,\n",
    "    min_model_comparisons=1000,\n",
    "    seed=42,\n",
    "    print_every=1,\n",
    "    \n",
    "    embedding_model_epochs=250, \n",
    "    embedding_spec=AttentionEmbeddingSpec(\n",
    "        encoder_model_name=\"all-MiniLM-L6-v2\",\n",
    "        optimizer=AdamWSpec(\n",
    "            learning_rate=2e-5,\n",
    "            lr_decay_gamma=0.995,\n",
    "        ),\n",
    "        h_emb=256,\n",
    "        h_scalar=64,\n",
    "        h_pair=256,\n",
    "        d_out=128,\n",
    "        pair_mlp_layers=8,\n",
    "        num_attention_heads=8,\n",
    "        dropout=0.1,\n",
    "        temperature=0.07,\n",
    "        pairs_per_model=128,\n",
    "        models_per_batch=8,\n",
    "    ),\n",
    "    base_model_name=\"gradient_boosting/gradient-boosting-model-embeddings-only\"\n",
    ")\n",
    "\n",
    "gradient_boosting_model.train(data, validation_split=ValidationSplit(val_fraction=0.2, seed=42), epochs=500, batch_size=128)\n",
    "gradient_boosting_model.save(\"gradient-boosting-model-embeddings-only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce526da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.dn_embedding_model import DnEmbeddingModel\n",
    "from src.models.finetuning_specs.lora_spec import LoraSpec\n",
    "from src.models.finetuning_specs.qlora_spec import QLoraSpec\n",
    "from src.models.optimizers.muon_spec import MuonSpec\n",
    "from src.models.transformer_embedding_model import TransformerEmbeddingModel\n",
    "from src.models.embedding_specs.attention_embedding_spec import AttentionEmbeddingSpec\n",
    "from src.models.optimizers.adamw_spec import AdamWSpec\n",
    "from src.utils.data_split import ValidationSplit\n",
    "\n",
    "from src.utils.timer import Timer\n",
    "#Timer.default_verbosity = None\n",
    "\n",
    "\n",
    "model_name = \"transformer-embedding-lora-mpnet-base-chatbot-arena\"\n",
    "model = TransformerEmbeddingModel(\n",
    "    transformer_model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
    "    hidden_dims=[256, 128, 64],\n",
    "    dropout=0.2,\n",
    "    max_length=256,\n",
    "    optimizer_spec=MuonSpec(\n",
    "        learning_rate=2e-2,\n",
    "        adamw_lr=1e-4,\n",
    "        lr_decay_gamma=0.99,\n",
    "        weight_decay=0.001,\n",
    "    ),\n",
    "    finetuning_spec=LoraSpec(\n",
    "        rank=16,\n",
    "        alpha=32,\n",
    "        dropout=0.05,\n",
    "        target_modules=[\"q\", \"v\"]\n",
    "    ),\n",
    "    balance_model_samples=True,\n",
    "    min_model_comparisons=1000,\n",
    "    seed=42,\n",
    "    print_every=1,\n",
    "    save_every=4,\n",
    "    checkpoint_name=model_name,\n",
    "    \n",
    "    embedding_model_epochs=250,\n",
    "    embedding_spec=AttentionEmbeddingSpec(\n",
    "        encoder_model_name=\"all-MiniLM-L6-v2\",\n",
    "        optimizer=AdamWSpec(\n",
    "            learning_rate=2e-5,\n",
    "            lr_decay_gamma=0.995,\n",
    "        ),\n",
    "        h_emb=256,\n",
    "        h_scalar=64,\n",
    "        h_pair=256,\n",
    "        d_out=128,\n",
    "        pair_mlp_layers=8,\n",
    "        num_attention_heads=8,\n",
    "        dropout=0.1,\n",
    "        temperature=0.07,\n",
    "        pairs_per_model=128,\n",
    "        models_per_batch=8,\n",
    "    )\n",
    ")\n",
    "\n",
    "model.train(data, validation_split=ValidationSplit(val_fraction=0.2, seed=42), epochs=40, batch_size=8)\n",
    "model.save(model_name)\n",
    "\n",
    "print()\n",
    "model.last_timer.inspect(1)\n",
    "#print()\n",
    "#model.embedding_model.last_timer.inspect(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff823cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.plotting_utils import plot_loss, plot_accuracy\n",
    "\n",
    "history = prompt_invariant_model.get_history()\n",
    "_, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "plot_loss(axes[0, 0], history.total_loss, \"Training loss\")\n",
    "plot_loss(axes[0, 1], history.val_loss, \"Validation loss\")\n",
    "plot_accuracy(axes[1, 0], history.train_accuracy, \"Training accuracy\")\n",
    "plot_accuracy(axes[1, 1], history.val_accuracy, \"Validation accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db5ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.plotting_utils import plot_loss, plot_accuracy\n",
    "\n",
    "embedding_history = model.embedding_model._epoch_logs\n",
    "\n",
    "_, axes = plt.subplots(3, 2, figsize=(15, 15))\n",
    "\n",
    "plot_loss(axes[0, 0], [e.train_loss for e in embedding_history], \"Training loss\")\n",
    "plot_loss(axes[0, 1], [e.val_loss for e in embedding_history], \"Validation loss\")\n",
    "plot_accuracy(axes[1, 0], [e.triplet_accuracy for e in embedding_history], \"Training triplet accuracy\")\n",
    "plot_accuracy(axes[1, 1], [e.val_triplet_accuracy for e in embedding_history], \"Validation triplet accuracy\")\n",
    "# plot_accuracy(axes[2, 0], [e.nearest_neighbor_accuracy for e in embedding_history], \"Training nearest neighbor accuracy\")\n",
    "# plot_accuracy(axes[2, 1], [e.val_nearest_neighbor_accuracy for e in embedding_history], \"Validation nearest neighbor accuracy\")\n",
    "plot_accuracy(axes[2, 0], [e.train_universal_accuracy for e in embedding_history], \"Training universal accuracy\")\n",
    "plot_accuracy(axes[2, 1], [e.val_universal_accuracy for e in embedding_history], \"Validation universal accuracy\")\n",
    "\n",
    "# axes[2, 0].plot([e.train_loss for e in embedding_history], label=\"Training loss\")\n",
    "# axes[2, 0].plot([e.train_triplet_loss for e in embedding_history], label=\"Training triplet loss\")\n",
    "# axes[2, 0].plot([e.train_reg_loss for e in embedding_history], label=\"Training regularization loss\")\n",
    "# axes[2, 0].legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
