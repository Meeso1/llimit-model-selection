{
  "data": {
    "max_samples": null,
    "validation_split": 0.2,
    "seed": 42
  },
  "epochs": 20,
  "batch_size": 32,
  "model": {
    "name": "transformer-embedding-lora",
    "spec": {
      "model_type": "transformer_embedding",
      "transformer_model_name": "sentence-transformers/all-MiniLM-L12-v2",
      "finetuning_spec": {
        "method": "lora",
        "rank": 16,
        "alpha": 32,
        "dropout": 0.05,
        "target_modules": "auto"
      },
      "hidden_dims": [256, 128, 64],
      "dropout": 0.2,
      "max_length": 256,
      "optimizer": {
        "optimizer_type": "adamw",
        "learning_rate": 1e-4,
        "lr_decay_gamma": 0.995,
        "weight_decay": 1e-2
      },
      "balance_model_samples": true,
      "embedding_spec": {
        "embedding_type": "attention",
        "encoder_model_name": "all-MiniLM-L6-v2",
        "h_emb": 128,
        "h_scalar": 32,
        "h_pair": 128,
        "d_out": 64,
        "pair_mlp_layers": 8,
        "num_attention_heads": 8,
        "dropout": 0.1,
        "temperature": 0.07,
        "pairs_per_model": 64,
        "models_per_batch": 8,
        "embeddings_per_model": 4,
        "optimizer": {
          "optimizer_type": "adamw",
          "learning_rate": 1e-4,
          "lr_decay_gamma": 0.995,
          "weight_decay": 1e-2
        }
      },
      "load_embedding_model_from": null,
      "min_model_comparisons": 1000,
      "embedding_model_epochs": 200,
      "seed": 42
    }
  },
  "log": {
    "run_name": "transformer-embedding-lora-default",
    "print_every": 1
  }
}

